{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#=========================================================================\n",
    "import tensorflow as tf\n",
    "#=========================================================================\n",
    "#网络结构定义\n",
    "    #输入参数：images，image batch、4D tensor、tf.float32、[batch_size, width, height, channels]\n",
    "    #返回参数：logits, float、 [batch_size, n_classes]\n",
    "def inference(images, batch_size, n_classes):\n",
    "#一个简单的卷积神经网络，卷积+池化层x2，全连接层x2，最后一个softmax层做分类。\n",
    "\n",
    "#卷积层1\n",
    "\n",
    "#64个3x3的卷积核（3通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[3,3,3,64], stddev = 1.0, dtype = tf.float32), \n",
    "                              name = 'weights', dtype = tf.float32)\n",
    "\n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [64]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name= scope.name)\n",
    "\n",
    "#池化层1\n",
    "\n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，局部响应归一化，对训练有利。\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')\n",
    "\n",
    "#卷积层2\n",
    "#16个3x3的卷积核（16通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[3,3,64,16], stddev = 0.1, dtype = tf.float32), \n",
    "                              name = 'weights', dtype = tf.float32)\n",
    "\n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [16]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides = [1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "\n",
    "#池化层2\n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，\n",
    "    #pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,beta=0.75,name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME',name='pooling2')\n",
    "\n",
    "#全连接层3\n",
    "#128个神经元，将之前pool层的输出reshape成一行，激活函数relu()\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[dim,128], stddev = 0.005, dtype = tf.float32),\n",
    "                             name = 'weights', dtype = tf.float32)\n",
    "\n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]), \n",
    "                             name = 'biases', dtype=tf.float32)\n",
    "\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "#全连接层4\n",
    "#128个神经元，激活函数relu() \n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[128,128], stddev = 0.005, dtype = tf.float32),\n",
    "                              name = 'weights',dtype = tf.float32)\n",
    "\n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "\n",
    "#dropout层        \n",
    "#    with tf.variable_scope('dropout') as scope:\n",
    "#        drop_out = tf.nn.dropout(local4, 0.8)\n",
    "\n",
    "\n",
    "#Softmax回归层\n",
    "#将前面的FC层输出，做一个线性回归，计算出每一类的得分，在这里是2类，所以这个层输出的是两个得分。\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[128, n_classes], stddev = 0.005, dtype = tf.float32),\n",
    "                              name = 'softmax_linear', dtype = tf.float32)\n",
    "\n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [n_classes]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    "\n",
    "    return softmax_linear\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "#loss计算\n",
    "    #传入参数：logits，网络计算输出值。labels，真实值，在这里是0或者1\n",
    "    #返回参数：loss，损失值\n",
    "def losses(logits, labels):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#loss损失值优化\n",
    "    #输入参数：loss。learning_rate，学习速率。\n",
    "    #返回参数：train_op，训练op，这个参数要输入sess.run中让模型去训练。\n",
    "def trainning(loss, learning_rate):\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "#评价/准确率计算\n",
    "    #输入参数：logits，网络计算值。labels，标签，也就是真实值，在这里是0或者1。\n",
    "    #返回参数：accuracy，当前step的平均准确率，也就是在这些batch中多少张图片被正确分类了。\n",
    "def evaluation(logits, labels):\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
